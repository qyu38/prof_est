{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/qyu38/Code/RNN_prof_est/npy_all_five/X_all_route_aligned_2000_seq_100_train_hatch_only_prof_vel.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 29\u001b[0m\n\u001b[0;32m     23\u001b[0m y_filename_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/y_all_route_aligned_2000_seq_100_test_hatch_only_prof_vel.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     26\u001b[0m model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/qyu38/Code/RNN_prof_est/model/model_all_route_sim_seq_100_FCL_hatch_only_LSTM_prof_vel_lr0d0001_epoch_200_transfer.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 29\u001b[0m X_standardized_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_filename_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# breakpoint()\u001b[39;00m\n\u001b[0;32m     31\u001b[0m X_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdelete(X_standardized_train, \u001b[38;5;241m0\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m#to leave out the simulated response\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\qyu38\\AppData\\Local\\anaconda3\\envs\\FNO\\lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/qyu38/Code/RNN_prof_est/npy_all_five/X_all_route_aligned_2000_seq_100_train_hatch_only_prof_vel.npy'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Specify the actual folder and filenames\n",
    "folder_path = 'C:/Users/qyu38/Code/RNN_prof_est/npy_all_five'  # Replace with your actual folder path\n",
    "# folder_path_test = 'C:/Users/qyu38/Code/RNN_prof_est/npy'\n",
    "# X_filename = '/X_standardized.npy' % this is the simulated data\n",
    "# y_filename = '/y.npy' % this is the simulated data\n",
    "\n",
    "X_filename_train = '/X_all_route_aligned_2000_seq_100_train_hatch_only_prof_vel.npy'\n",
    "y_filename_train = '/y_all_route_aligned_2000_seq_100_train_hatch_only_prof_vel.npy'\n",
    "\n",
    "X_filename_val = '/X_all_route_aligned_2000_seq_100_validate_hatch_only_prof_vel.npy'\n",
    "y_filename_val = '/y_all_route_aligned_2000_seq_100_validate_hatch_only_prof_vel.npy'\n",
    "\n",
    "X_filename_test = '/X_all_route_aligned_2000_seq_100_test_hatch_only_prof_vel.npy'\n",
    "y_filename_test = '/y_all_route_aligned_2000_seq_100_test_hatch_only_prof_vel.npy'\n",
    "\n",
    "\n",
    "model_save_path = 'C:/Users/qyu38/Code/RNN_prof_est/model/model_all_route_sim_seq_100_FCL_hatch_only_LSTM_prof_vel_lr0d0001_epoch_200_transfer.pth'\n",
    "\n",
    "\n",
    "X_standardized_train = np.load(folder_path + X_filename_train)\n",
    "# breakpoint()\n",
    "X_train = np.delete(X_standardized_train, 0, axis=2) #to leave out the simulated response\n",
    "# X_train = np.delete(X_standardized_train, -1, axis=2) #to leave out the real response, train using simulated\n",
    "\n",
    "X_standardized_val = np.load(folder_path + X_filename_val)\n",
    "X_val = np.delete(X_standardized_val, 0, axis=2) #to leave out the simulated response\n",
    "# X_val = np.delete(X_standardized_val, -1, axis=2) #to leave out the real response, train using simulated\n",
    "\n",
    "X_standardized_test = np.load(folder_path + X_filename_test)\n",
    "X_test = np.delete(X_standardized_test, 0, axis=2) #to leave out the simulated response\n",
    "# X_test = np.delete(X_standardized_test, -1, axis=2) #to leave out the real response, testing using simulated\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "y_train = np.load(folder_path + y_filename_train)\n",
    "y_val = np.load(folder_path + y_filename_val)\n",
    "y_test = np.load(folder_path + y_filename_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LSTM model\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=2, hidden_size=100, num_layers=4, batch_first=True)\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(100, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)  # LSTM output is (output, (h_n, c_n)), we only need the output\n",
    "        x = x[:, -1, :]  # Get the last time step output from LSTM\n",
    "        out = self.fc_layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters of the first three LSTM layers are frozen:\n",
      "lstm.weight_ih_l0 is frozen\n",
      "lstm.weight_hh_l0 is frozen\n",
      "lstm.bias_ih_l0 is frozen\n",
      "lstm.bias_hh_l0 is frozen\n",
      "lstm.weight_ih_l1 is frozen\n",
      "lstm.weight_hh_l1 is frozen\n",
      "lstm.bias_ih_l1 is frozen\n",
      "lstm.bias_hh_l1 is frozen\n",
      "lstm.weight_ih_l2 is frozen\n",
      "lstm.weight_hh_l2 is frozen\n",
      "lstm.bias_ih_l2 is frozen\n",
      "lstm.bias_hh_l2 is frozen\n"
     ]
    }
   ],
   "source": [
    "# Define the path to your pre-trained model (trained on simulation data)\n",
    "pretrained_model_path = r'C:/Users/qyu38/Code/RNN_prof_est/model/model_all_route_sim_seq_100_FCL_hatch_only_LSTM_prof_vel_lr0d001_epoch_200.pth'\n",
    "\n",
    "# Initialize the model architecture\n",
    "net = LSTMNet()\n",
    "\n",
    "# Load the pre-trained weights into the model\n",
    "net.load_state_dict(torch.load(pretrained_model_path))\n",
    "\n",
    "# Freeze the first three hidden layers (l0, l1, l2) of the LSTM\n",
    "for name, param in net.lstm.named_parameters():\n",
    "    if 'l0' in name or 'l1' in name or 'l2' in name:\n",
    "        param.requires_grad = False  # Freeze the parameters for the first three layers\n",
    "\n",
    "# Check which parameters are frozen\n",
    "print(\"Parameters of the first three LSTM layers are frozen:\")\n",
    "for name, param in net.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        print(f\"{name} is frozen\")\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "# Initialize optimizer (only train the unfrozen parameters)\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training loader length is: 14640\n",
      "Epoch 1/400, Training Loss: 0.0011704618751079822, Validation Loss: 0.0013870218224177379\n",
      "Epoch 2/400, Training Loss: 0.0009908896814313714, Validation Loss: 0.0013151397359222584\n",
      "Epoch 3/400, Training Loss: 0.0009397877534080736, Validation Loss: 0.001297666022118679\n",
      "Epoch 4/400, Training Loss: 0.0009109157577151931, Validation Loss: 0.0012828986959043216\n",
      "Epoch 5/400, Training Loss: 0.0008885788827355412, Validation Loss: 0.001299909633866871\n",
      "Epoch 6/400, Training Loss: 0.0008715875497976617, Validation Loss: 0.0013026731926471797\n",
      "Epoch 7/400, Training Loss: 0.0008571502199738659, Validation Loss: 0.0012953186939374788\n",
      "Epoch 8/400, Training Loss: 0.0008444669429787569, Validation Loss: 0.001317413432323132\n",
      "Epoch 9/400, Training Loss: 0.0008334001693086568, Validation Loss: 0.0013164076204873441\n",
      "Epoch 10/400, Training Loss: 0.000821971761857252, Validation Loss: 0.0013220893252056211\n",
      "Epoch 11/400, Training Loss: 0.0008126721378359604, Validation Loss: 0.0013237359321466074\n",
      "Epoch 12/400, Training Loss: 0.0008032768879205376, Validation Loss: 0.0013245543132451843\n",
      "Epoch 13/400, Training Loss: 0.0007941222346789055, Validation Loss: 0.0013656872805095752\n",
      "Epoch 14/400, Training Loss: 0.0007863271154881241, Validation Loss: 0.0013572721528253492\n",
      "Epoch 15/400, Training Loss: 0.0007788800057873818, Validation Loss: 0.0013472123475394484\n",
      "Epoch 16/400, Training Loss: 0.0007713744750692029, Validation Loss: 0.001334779557872645\n",
      "Epoch 17/400, Training Loss: 0.0007644023653314623, Validation Loss: 0.00136655788181554\n",
      "Epoch 18/400, Training Loss: 0.000758209119575455, Validation Loss: 0.0013850282857165073\n",
      "Epoch 19/400, Training Loss: 0.0007520634373239241, Validation Loss: 0.0013777880040871699\n",
      "Epoch 20/400, Training Loss: 0.00074611501731951, Validation Loss: 0.0013601403998170889\n",
      "Epoch 21/400, Training Loss: 0.0007406250594399377, Validation Loss: 0.0014031713052803697\n",
      "Epoch 22/400, Training Loss: 0.0007352711149135762, Validation Loss: 0.001386033005898344\n",
      "Epoch 23/400, Training Loss: 0.0007297337835006062, Validation Loss: 0.0014013547574812217\n",
      "Epoch 24/400, Training Loss: 0.0007249480974941996, Validation Loss: 0.001384362336216358\n",
      "Epoch 25/400, Training Loss: 0.0007200673469079482, Validation Loss: 0.00140372452795299\n",
      "Epoch 26/400, Training Loss: 0.0007152234351441685, Validation Loss: 0.0014084391393578714\n",
      "Epoch 27/400, Training Loss: 0.0007117612561135209, Validation Loss: 0.0014200354420131488\n",
      "Epoch 28/400, Training Loss: 0.0007059778719034353, Validation Loss: 0.0014202203379009744\n",
      "Epoch 29/400, Training Loss: 0.0007022290624656029, Validation Loss: 0.0014425277045024194\n",
      "Epoch 30/400, Training Loss: 0.0006982489948835583, Validation Loss: 0.0014092129017355861\n",
      "Epoch 31/400, Training Loss: 0.0006943984798140934, Validation Loss: 0.0013906767055049836\n",
      "Epoch 32/400, Training Loss: 0.0006900464909116526, Validation Loss: 0.001437618584936747\n",
      "Epoch 33/400, Training Loss: 0.0006870790784072438, Validation Loss: 0.0014325911734553382\n",
      "Epoch 34/400, Training Loss: 0.000683329068082547, Validation Loss: 0.0014045493241083386\n",
      "Epoch 35/400, Training Loss: 0.0006796603989462964, Validation Loss: 0.0014383364683056526\n",
      "Epoch 36/400, Training Loss: 0.000676256831018472, Validation Loss: 0.001430702888093195\n",
      "Epoch 37/400, Training Loss: 0.0006730692029237328, Validation Loss: 0.0014739594944071497\n",
      "Epoch 38/400, Training Loss: 0.0006696556635802774, Validation Loss: 0.0014680922483331688\n",
      "Epoch 39/400, Training Loss: 0.0006675882247736063, Validation Loss: 0.001444882094708045\n",
      "Epoch 40/400, Training Loss: 0.0006631981721809888, Validation Loss: 0.0014870379260667108\n",
      "Epoch 41/400, Training Loss: 0.0006603291486096511, Validation Loss: 0.0014467340413474698\n",
      "Epoch 42/400, Training Loss: 0.0006574892739975991, Validation Loss: 0.001440589373004402\n",
      "Epoch 43/400, Training Loss: 0.0006545250672582474, Validation Loss: 0.0014724423187841207\n",
      "Epoch 44/400, Training Loss: 0.0006522617041039671, Validation Loss: 0.0014550228489975854\n",
      "Epoch 45/400, Training Loss: 0.0006488267017878774, Validation Loss: 0.0014621953707136977\n",
      "Epoch 46/400, Training Loss: 0.0006471273991377272, Validation Loss: 0.001465911203191394\n",
      "Epoch 47/400, Training Loss: 0.0006443894389067182, Validation Loss: 0.0014695359502999111\n",
      "Epoch 48/400, Training Loss: 0.0006412217607303419, Validation Loss: 0.0014792381693920166\n",
      "Epoch 49/400, Training Loss: 0.0006402431533114808, Validation Loss: 0.001491833280112171\n",
      "Epoch 50/400, Training Loss: 0.0006360800271485474, Validation Loss: 0.0014798867704794885\n",
      "Epoch 51/400, Training Loss: 0.0006343073763102061, Validation Loss: 0.0014917295636100295\n",
      "Epoch 52/400, Training Loss: 0.0006329184783462154, Validation Loss: 0.001481943829150564\n",
      "Epoch 53/400, Training Loss: 0.0006294676204491677, Validation Loss: 0.0014808523595736536\n",
      "Epoch 54/400, Training Loss: 0.000629089555904564, Validation Loss: 0.0014802200449427225\n",
      "Epoch 55/400, Training Loss: 0.0006262839935854194, Validation Loss: 0.0014969406712998456\n",
      "Epoch 56/400, Training Loss: 0.0006235390289599865, Validation Loss: 0.0015246578465397563\n",
      "Epoch 57/400, Training Loss: 0.0006220933914593217, Validation Loss: 0.0014938274703621501\n",
      "Epoch 58/400, Training Loss: 0.0006196232903626291, Validation Loss: 0.0014727287098025006\n",
      "Epoch 59/400, Training Loss: 0.0006167343855571734, Validation Loss: 0.0015086800469077415\n",
      "Epoch 60/400, Training Loss: 0.0006159755807872347, Validation Loss: 0.0014893437216095858\n",
      "Epoch 61/400, Training Loss: 0.0006130638915332224, Validation Loss: 0.001514262504593533\n",
      "Epoch 62/400, Training Loss: 0.000611041020232226, Validation Loss: 0.0015252717956836825\n",
      "Epoch 63/400, Training Loss: 0.0006091443163711264, Validation Loss: 0.0014994499021012274\n",
      "Epoch 64/400, Training Loss: 0.0006086303824686879, Validation Loss: 0.0015213866975812406\n",
      "Epoch 65/400, Training Loss: 0.0006051598317579868, Validation Loss: 0.0015414751403293834\n",
      "Epoch 66/400, Training Loss: 0.0006034157238214813, Validation Loss: 0.001482132160299875\n",
      "Epoch 67/400, Training Loss: 0.0006022734141193633, Validation Loss: 0.0014966725249596384\n",
      "Epoch 68/400, Training Loss: 0.0005999515103933254, Validation Loss: 0.0015439878440044509\n",
      "Epoch 69/400, Training Loss: 0.0005985771317409174, Validation Loss: 0.0015161737951403986\n",
      "Epoch 70/400, Training Loss: 0.0005961549810497354, Validation Loss: 0.0015132961973431632\n",
      "Epoch 71/400, Training Loss: 0.0005942279437461918, Validation Loss: 0.0015151899441552081\n",
      "Epoch 72/400, Training Loss: 0.0005930184259868142, Validation Loss: 0.0015483434040327992\n",
      "Epoch 73/400, Training Loss: 0.0005919496420002892, Validation Loss: 0.0014898259625814396\n",
      "Epoch 74/400, Training Loss: 0.0005904942055579843, Validation Loss: 0.0015256273415660572\n",
      "Epoch 75/400, Training Loss: 0.0005884963051506869, Validation Loss: 0.0015333365473567595\n",
      "Epoch 76/400, Training Loss: 0.0005867813341353357, Validation Loss: 0.0015218345320443746\n",
      "Epoch 77/400, Training Loss: 0.0005851425611745369, Validation Loss: 0.0015412736477332179\n",
      "Epoch 78/400, Training Loss: 0.0005832680284931952, Validation Loss: 0.0016111939636282747\n",
      "Epoch 79/400, Training Loss: 0.0005816230219829404, Validation Loss: 0.0015306960789685973\n",
      "Epoch 80/400, Training Loss: 0.0005807540553978271, Validation Loss: 0.00155021398481377\n",
      "Epoch 81/400, Training Loss: 0.0005794123441470825, Validation Loss: 0.001544835567314568\n",
      "Epoch 82/400, Training Loss: 0.0005773401502019276, Validation Loss: 0.0015320445124330196\n",
      "Epoch 83/400, Training Loss: 0.000576487072919649, Validation Loss: 0.0015578961505393594\n",
      "Epoch 84/400, Training Loss: 0.0005749902439898416, Validation Loss: 0.001567959750636438\n",
      "Epoch 85/400, Training Loss: 0.0005730375798352005, Validation Loss: 0.0015429401937898583\n",
      "Epoch 86/400, Training Loss: 0.0005709481663413742, Validation Loss: 0.001542240687571076\n",
      "Epoch 87/400, Training Loss: 0.0005713474936127284, Validation Loss: 0.0015794106384697788\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     45\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 47\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Validation loss\u001b[39;00m\n\u001b[0;32m     50\u001b[0m net\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Set the device to GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "net.to(device)  # Move the model to the specified device\n",
    "\n",
    "batch_size = 64  # You can adjust the batch size\n",
    "\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_data = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())\n",
    "val_data = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).float())\n",
    "test_data = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float())\n",
    "# test_data_real = TensorDataset(torch.from_numpy(X_test_real).float(), torch.from_numpy(y_test).float())\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "# test_loader_real = DataLoader(test_data_real, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "# Set the number of epochs\n",
    "num_epochs = 400  # You can adjust this\n",
    "# print(len(train_loader))\n",
    "print(f\"Training loader length is: {len(train_loader)}\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to the device\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "        # print(f\"Labels shape: {labels.shape}\")  # Debugging statement\n",
    "        # print(f\"Outputs shape: {outputs.shape}\")  # Debugging statement\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validation loss\n",
    "    net.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to device/and can be deleted\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {running_loss/len(train_loader)}, Validation Loss: {val_loss/len(val_loader)}\")\n",
    "\n",
    "# Save the model state dict\n",
    "torch.save(net.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "\n",
    "################################### Start the testing process ############################################################################################\n",
    "net.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to the device\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "print(f\"Test Loss: {test_loss/len(test_loader)}\")\n",
    "\n",
    "\n",
    "predictions = []\n",
    "ground_truth = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)  # Move inputs to the same device as the model\n",
    "        outputs = net(inputs)\n",
    "        predictions.extend(outputs.cpu().numpy())\n",
    "        ground_truth.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert lists to arrays for plotting\n",
    "predictions = np.array(predictions)\n",
    "ground_truth = np.array(ground_truth)\n",
    "\n",
    "mse = mean_squared_error(ground_truth, predictions)\n",
    "print(f\"mean_squared_error: {mse}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(ground_truth, label='Ground Truth')\n",
    "plt.plot(predictions, label='Predicted')\n",
    "plt.title('Comparison of Ground Truth and Predictions')\n",
    "plt.xlabel('profile points')\n",
    "plt.ylabel('roughness geometry (m)')\n",
    "plt.legend()\n",
    "########################################################################################################################################################################################\n",
    "# Save the figure\n",
    "# plt.savefig('C:/Users/qyu38/Code/RNN_prof_est/data/test_data_plot_real.png')  # Change the path and file name as needed\n",
    "# import pickle\n",
    "# # Save the figure\n",
    "# with open('saved_figure.pkl', 'wb') as file:\n",
    "#     pickle.dump(plt.gcf(), file)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "breakpoint()\n",
    "# import pandas as pd\n",
    "# file_path = r\"C:\\Users\\qyu38\\Code\\RNN_prof_est\\performance_plot\\predictions_sim_seq_100.csv\"\n",
    "# file_path_ground = r\"C:\\Users\\qyu38\\Code\\RNN_prof_est\\performance_plot\\ground_truth_sim_seq_100.csv\"\n",
    "# df_ground = pd.DataFrame(ground_truth)\n",
    "# df_predict = pd.DataFrame(predictions)\n",
    "\n",
    "# df_predict.to_csv(file_path, index=False, header=False)\n",
    "# df_ground.to_csv(file_path_ground, index=False, header=False)\n",
    "# breakpoint()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FNO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
